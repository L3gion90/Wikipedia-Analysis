In this project, carried out with pyspark through databricks community, we will deal with an EDA on a dataset taken from wikipedia, and following textual classifier training. 

Specifically, we will calculate the number of articles, the average number of words used, the maximum number of words present in the longest article, the minimum number of words present in the shortest article, and for each category we will identify the most representative word cloud.

There will be two classifiers, one for the summary column and one for the documents column, so that we can check which has greater accuracy for our problem.
